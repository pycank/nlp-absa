{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9795389,"sourceType":"datasetVersion","datasetId":6002825},{"sourceId":10052256,"sourceType":"datasetVersion","datasetId":6193753},{"sourceId":10052270,"sourceType":"datasetVersion","datasetId":6193765}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install datasets transformers simpletransformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T20:11:19.569683Z","iopub.execute_input":"2024-12-20T20:11:19.570008Z","iopub.status.idle":"2024-12-20T20:11:33.442803Z","shell.execute_reply.started":"2024-12-20T20:11:19.569968Z","shell.execute_reply":"2024-12-20T20:11:33.441983Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.0.1)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.1)\nCollecting simpletransformers\n  Downloading simpletransformers-0.70.1-py3-none-any.whl.metadata (42 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.15.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: huggingface-hub>=0.22.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.25.1)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.0)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from simpletransformers) (1.14.1)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from simpletransformers) (1.2.2)\nCollecting seqeval (from simpletransformers)\n  Downloading seqeval-1.2.2.tar.gz (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: tensorboard in /opt/conda/lib/python3.10/site-packages (from simpletransformers) (2.16.2)\nRequirement already satisfied: tensorboardx in /opt/conda/lib/python3.10/site-packages (from simpletransformers) (2.6.2.2)\nRequirement already satisfied: wandb>=0.10.32 in /opt/conda/lib/python3.10/site-packages (from simpletransformers) (0.18.3)\nCollecting streamlit (from simpletransformers)\n  Downloading streamlit-1.41.1-py2.py3-none-any.whl.metadata (8.5 kB)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from simpletransformers) (0.2.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb>=0.10.32->simpletransformers) (8.1.7)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb>=0.10.32->simpletransformers) (0.4.0)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb>=0.10.32->simpletransformers) (3.1.43)\nRequirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from wandb>=0.10.32->simpletransformers) (3.11.0)\nRequirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb>=0.10.32->simpletransformers) (3.20.3)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb>=0.10.32->simpletransformers) (5.9.3)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb>=0.10.32->simpletransformers) (2.15.0)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb>=0.10.32->simpletransformers) (1.3.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb>=0.10.32->simpletransformers) (70.0.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->simpletransformers) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->simpletransformers) (3.5.0)\nRequirement already satisfied: altair<6,>=4.0 in /opt/conda/lib/python3.10/site-packages (from streamlit->simpletransformers) (5.4.1)\nRequirement already satisfied: blinker<2,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from streamlit->simpletransformers) (1.8.2)\nRequirement already satisfied: cachetools<6,>=4.0 in /opt/conda/lib/python3.10/site-packages (from streamlit->simpletransformers) (4.2.4)\nRequirement already satisfied: pillow<12,>=7.1.0 in /opt/conda/lib/python3.10/site-packages (from streamlit->simpletransformers) (10.3.0)\nRequirement already satisfied: rich<14,>=10.14.0 in /opt/conda/lib/python3.10/site-packages (from streamlit->simpletransformers) (13.7.1)\nRequirement already satisfied: tenacity<10,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from streamlit->simpletransformers) (8.3.0)\nRequirement already satisfied: toml<2,>=0.10.1 in /opt/conda/lib/python3.10/site-packages (from streamlit->simpletransformers) (0.10.2)\nRequirement already satisfied: watchdog<7,>=2.1.5 in /opt/conda/lib/python3.10/site-packages (from streamlit->simpletransformers) (5.0.3)\nCollecting pydeck<1,>=0.8.0b4 (from streamlit->simpletransformers)\n  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\nRequirement already satisfied: tornado<7,>=6.0.3 in /opt/conda/lib/python3.10/site-packages (from streamlit->simpletransformers) (6.4.1)\nRequirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.10/site-packages (from tensorboard->simpletransformers) (1.4.0)\nRequirement already satisfied: grpcio>=1.48.2 in /opt/conda/lib/python3.10/site-packages (from tensorboard->simpletransformers) (1.62.2)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard->simpletransformers) (3.6)\nRequirement already satisfied: six>1.9 in /opt/conda/lib/python3.10/site-packages (from tensorboard->simpletransformers) (1.16.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard->simpletransformers) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard->simpletransformers) (3.0.4)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit->simpletransformers) (3.1.4)\nRequirement already satisfied: jsonschema>=3.0 in /opt/conda/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit->simpletransformers) (4.22.0)\nRequirement already satisfied: narwhals>=1.5.2 in /opt/conda/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit->simpletransformers) (1.9.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.10.32->simpletransformers) (4.0.11)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich<14,>=10.14.0->streamlit->simpletransformers) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich<14,>=10.14.0->streamlit->simpletransformers) (2.18.0)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard->simpletransformers) (2.1.5)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.10.32->simpletransformers) (5.0.1)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->simpletransformers) (2023.12.1)\nRequirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->simpletransformers) (0.35.1)\nRequirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->simpletransformers) (0.18.1)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit->simpletransformers) (0.1.2)\nDownloading simpletransformers-0.70.1-py3-none-any.whl (316 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.3/316.3 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading streamlit-1.41.1-py2.py3-none-any.whl (9.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m100.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m101.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: seqeval\n  Building wheel for seqeval (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16161 sha256=453b6786d858d91f64ea7429a4dd70e1370d29059afcfb73b175d51c06f7a584\n  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\nSuccessfully built seqeval\nInstalling collected packages: pydeck, seqeval, streamlit, simpletransformers\nSuccessfully installed pydeck-0.9.1 seqeval-1.2.2 simpletransformers-0.70.1 streamlit-1.41.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport ast\nimport torch\nfrom datasets import load_dataset, Dataset\nfrom simpletransformers.config.model_args import Seq2SeqArgs\nfrom torch.utils.data import DataLoader, RandomSampler\nfrom tqdm.auto import tqdm, trange\nfrom transformers import (\n    AdamW,\n    AutoModel,\n    AutoTokenizer,\n    BartForConditionalGeneration,\n    get_scheduler\n)\nimport math","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T20:11:33.444928Z","iopub.execute_input":"2024-12-20T20:11:33.445207Z","iopub.status.idle":"2024-12-20T20:11:39.149788Z","shell.execute_reply.started":"2024-12-20T20:11:33.445179Z","shell.execute_reply":"2024-12-20T20:11:39.149101Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"#1. Read data ","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv(\"/kaggle/input/segmented-absa-vlsp-2018/datasets/1-VLSP2018-SA-Hotel-train.csv\")\ndf_test  = pd.read_csv(\"/kaggle/input/segmented-absa-vlsp-2018/datasets/3-VLSP2018-SA-Hotel-test.csv\")\ndf_test_small = df_test.head(60)\ncategories_list = df_train.columns[1:]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T20:11:39.150889Z","iopub.execute_input":"2024-12-20T20:11:39.151418Z","iopub.status.idle":"2024-12-20T20:11:39.241989Z","shell.execute_reply.started":"2024-12-20T20:11:39.151379Z","shell.execute_reply":"2024-12-20T20:11:39.241188Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"categories_list","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T20:11:39.242921Z","iopub.execute_input":"2024-12-20T20:11:39.243164Z","iopub.status.idle":"2024-12-20T20:11:39.249941Z","shell.execute_reply.started":"2024-12-20T20:11:39.243140Z","shell.execute_reply":"2024-12-20T20:11:39.249006Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"Index(['AMBIENCE#GENERAL', 'DRINKS#PRICES', 'DRINKS#QUALITY',\n       'DRINKS#STYLE&OPTIONS', 'FOOD#PRICES', 'FOOD#QUALITY',\n       'FOOD#STYLE&OPTIONS', 'LOCATION#GENERAL', 'RESTAURANT#GENERAL',\n       'RESTAURANT#MISCELLANEOUS', 'RESTAURANT#PRICES', 'SERVICE#GENERAL'],\n      dtype='object')"},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"#2. Preprocess data in accurate form","metadata":{}},{"cell_type":"code","source":"#template = \"\"\"Hãy xác định các hạng mục được đề cập trong nhận xét sau: '{input_text}'. Các hạng mục có thể là: {categories}. Chỉ liệt kê các hạng mục được đề cập.\n#\"\"\"\nvalue_mapping = {\n        0: \"none\",\n        1: 'positive',\n        2: 'negative',\n        3: 'neutral'\n    }\ndef build_absa_template(df):\n    rows = []\n    sentiment_columns = df_train.columns[1:]\n    for index, row in df_train.iterrows():\n        result_row = []\n        for col in sentiment_columns:\n            sentiment = row[col]\n            rows.append({\n                        \"review\": row[\"Review\"] ,\n                        \"template\": f\"Cảm xúc của hạng mục {col} là {value_mapping[sentiment]}.\",\n                    })\n            \n    # dataset = Dataset.from_pandas(pd.DataFrame(rows))\n\n    return pd.DataFrame(rows)\n\ndf_train['Review'] = df_train['Review'].apply(lambda x: ''.join(ast.literal_eval(x)))\ndf_test['Review'] = df_test['Review'].apply(lambda x: ''.join(ast.literal_eval(x)))\n\ntrain_df = build_absa_template(df_train)\ntrain_df = train_df.rename(columns={\"review\": \"input_text\", \"template\" : \"target_text\"})\ntrain_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T20:11:39.251163Z","iopub.execute_input":"2024-12-20T20:11:39.251480Z","iopub.status.idle":"2024-12-20T20:11:39.334147Z","shell.execute_reply.started":"2024-12-20T20:11:39.251454Z","shell.execute_reply":"2024-12-20T20:11:39.333341Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"                                             input_text  \\\n0     _ Ảnh chụp từ hôm_qua , đi chơi với gia_đình v...   \n1     _ Ảnh chụp từ hôm_qua , đi chơi với gia_đình v...   \n2     _ Ảnh chụp từ hôm_qua , đi chơi với gia_đình v...   \n3     _ Ảnh chụp từ hôm_qua , đi chơi với gia_đình v...   \n4     _ Ảnh chụp từ hôm_qua , đi chơi với gia_đình v...   \n...                                                 ...   \n3595  Ăn_ở đây cũng ngon , gà roti hay gà rán đều ng...   \n3596  Ăn_ở đây cũng ngon , gà roti hay gà rán đều ng...   \n3597  Ăn_ở đây cũng ngon , gà roti hay gà rán đều ng...   \n3598  Ăn_ở đây cũng ngon , gà roti hay gà rán đều ng...   \n3599  Ăn_ở đây cũng ngon , gà roti hay gà rán đều ng...   \n\n                                            target_text  \n0        Cảm xúc của hạng mục AMBIENCE#GENERAL là none.  \n1           Cảm xúc của hạng mục DRINKS#PRICES là none.  \n2          Cảm xúc của hạng mục DRINKS#QUALITY là none.  \n3     Cảm xúc của hạng mục DRINKS#STYLE&OPTIONS là n...  \n4             Cảm xúc của hạng mục FOOD#PRICES là none.  \n...                                                 ...  \n3595     Cảm xúc của hạng mục LOCATION#GENERAL là none.  \n3596   Cảm xúc của hạng mục RESTAURANT#GENERAL là none.  \n3597  Cảm xúc của hạng mục RESTAURANT#MISCELLANEOUS ...  \n3598    Cảm xúc của hạng mục RESTAURANT#PRICES là none.  \n3599  Cảm xúc của hạng mục SERVICE#GENERAL là positive.  \n\n[3600 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>input_text</th>\n      <th>target_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>_ Ảnh chụp từ hôm_qua , đi chơi với gia_đình v...</td>\n      <td>Cảm xúc của hạng mục AMBIENCE#GENERAL là none.</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>_ Ảnh chụp từ hôm_qua , đi chơi với gia_đình v...</td>\n      <td>Cảm xúc của hạng mục DRINKS#PRICES là none.</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>_ Ảnh chụp từ hôm_qua , đi chơi với gia_đình v...</td>\n      <td>Cảm xúc của hạng mục DRINKS#QUALITY là none.</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>_ Ảnh chụp từ hôm_qua , đi chơi với gia_đình v...</td>\n      <td>Cảm xúc của hạng mục DRINKS#STYLE&amp;OPTIONS là n...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>_ Ảnh chụp từ hôm_qua , đi chơi với gia_đình v...</td>\n      <td>Cảm xúc của hạng mục FOOD#PRICES là none.</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3595</th>\n      <td>Ăn_ở đây cũng ngon , gà roti hay gà rán đều ng...</td>\n      <td>Cảm xúc của hạng mục LOCATION#GENERAL là none.</td>\n    </tr>\n    <tr>\n      <th>3596</th>\n      <td>Ăn_ở đây cũng ngon , gà roti hay gà rán đều ng...</td>\n      <td>Cảm xúc của hạng mục RESTAURANT#GENERAL là none.</td>\n    </tr>\n    <tr>\n      <th>3597</th>\n      <td>Ăn_ở đây cũng ngon , gà roti hay gà rán đều ng...</td>\n      <td>Cảm xúc của hạng mục RESTAURANT#MISCELLANEOUS ...</td>\n    </tr>\n    <tr>\n      <th>3598</th>\n      <td>Ăn_ở đây cũng ngon , gà roti hay gà rán đều ng...</td>\n      <td>Cảm xúc của hạng mục RESTAURANT#PRICES là none.</td>\n    </tr>\n    <tr>\n      <th>3599</th>\n      <td>Ăn_ở đây cũng ngon , gà roti hay gà rán đều ng...</td>\n      <td>Cảm xúc của hạng mục SERVICE#GENERAL là positive.</td>\n    </tr>\n  </tbody>\n</table>\n<p>3600 rows × 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"def build_absa_test(df):\n    rows = []\n\n    sentiment_columns = df.columns[1:]\n    for index, row in df.iterrows():\n        labels = []\n        for col in sentiment_columns:\n            sentiment = row[col]\n            labels.append(sentiment)\n            #if sentiment > 0:\n                #labels.append((col, value_mapping[sentiment]))\n        rows.append({\n                \"input_text\" : row[\"Review\"],\n                \"labels\": labels\n        })\n\n    return pd.DataFrame(rows)\n\ntest_df = build_absa_test(df_test)\ntest_df_small = build_absa_test(df_test_small)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T20:11:39.335784Z","iopub.execute_input":"2024-12-20T20:11:39.336152Z","iopub.status.idle":"2024-12-20T20:11:39.384838Z","shell.execute_reply.started":"2024-12-20T20:11:39.336114Z","shell.execute_reply":"2024-12-20T20:11:39.383966Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"#3. Validation function","metadata":{}},{"cell_type":"code","source":"from simpletransformers.seq2seq import Seq2SeqModel\nimport pandas as pd\nfrom transformers import BartForConditionalGeneration, BartTokenizer, BartConfig\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score,f1_score\n# logging.basicConfig(level=logging.INFO)\n# transformers_logger = logging.getLogger(\"transformers\")\n# transformers_logger.setLevel(logging.WARNING)\nimport torch\nimport numpy as np\n\ndef predict_val(model, device, test_df):\n    global categories_list\n    candidate_list = [\"none\", \"positive\", \"negative\", \"neutral\"]\n\n    # model = BartForConditionalGeneration.from_pretrained('./outputs/checkpoint-513-epoch-19')\n    model.eval()\n    model.config.use_cache = False\n    tokenizer = AutoTokenizer.from_pretrained('vinai/bartpho-syllable-base')\n    count = 0\n    total = 0\n    predictions_total = []\n    labels_total = []\n    for index, row in test_df.iterrows():\n        x, labels = row[\"input_text\"],row[\"labels\"]\n        input_ids = tokenizer([x] * 4, return_tensors='pt')['input_ids']\n        #labels_aspect = {label[0] : label[1] for label in labels}\n        #labels_idx = [candidate_list.index(labels_aspect[category]) if category in labels_aspect.keys() else 0 for category in categories_list)\n        predictions = []\n\n        for category in categories_list:\n            score_list = []\n            target_list = [f\"Cảm xúc của hạng mục {category} là {candi}.\" for candi in\n                            candidate_list]\n            \n            output_ids = tokenizer(target_list, return_tensors='pt', padding=True, truncation=True)['input_ids']\n            with torch.no_grad():\n                output = model(input_ids=input_ids.to(device), decoder_input_ids=output_ids.to(device))[0]\n                logits = output.softmax(dim=-1).to('cpu').numpy()\n            for i in range(4):\n                score = 1\n                for j in range(logits[i].shape[0] - 2):\n                    score *= logits[i][j][output_ids[i][j + 1]]\n                score_list.append(score)\n                \n            predict_category = np.argmax(score_list)\n            predictions.append(predict_category)\n\n        predictions_total.extend(predictions)\n        labels_total.extend(labels)\n    recall = recall_score(labels_total, predictions_total,average='macro')\n    precision = precision_score(labels_total, predictions_total,average='macro')\n    f1 = f1_score(labels_total, predictions_total, average='macro')\n    print(f'Recall: {recall}')\n    print(f'Precision: {precision}')\n    print(f'f1_score: {f1}')\n    return accuracy_score(labels_total, predictions_total)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T20:11:39.387905Z","iopub.execute_input":"2024-12-20T20:11:39.388193Z","iopub.status.idle":"2024-12-20T20:11:51.318108Z","shell.execute_reply.started":"2024-12-20T20:11:39.388167Z","shell.execute_reply":"2024-12-20T20:11:51.317398Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"#4. Specify the Focal Loss","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\nclass SequenceFocalLoss(nn.Module):\n    def __init__(self, alpha=1.0, gamma=4.0, reduction=\"mean\", ignore_index=-100):\n        \"\"\"\n        Focal Loss adapted for sequence generation tasks.\n\n        Parameters:\n        - alpha: Scaling factor for positive examples.\n        - gamma: Focusing parameter for hard examples.\n        - reduction: 'none' | 'mean' | 'sum'\n        - ignore_index: Token index to ignore (e.g., padding tokens in sequence tasks).\n        \"\"\"\n        super(SequenceFocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.reduction = reduction\n        self.ignore_index = ignore_index\n\n    def forward(self, logits, targets):\n        # logits: (batch_size, seq_len, vocab_size)\n        # targets: (batch_size, seq_len)\n        logits = logits.view(-1, logits.size(-1))  # Flatten to (N*vocab_size)\n        targets = targets.view(-1)  # Flatten to (N)\n\n        # Mask out the ignore_index\n        valid_indices = targets != self.ignore_index\n        logits = logits[valid_indices]\n        targets = targets[valid_indices]\n\n        # Compute CrossEntropyLoss for valid indices\n        ce_loss = nn.CrossEntropyLoss(reduction=\"none\")(logits, targets)\n        pt = torch.exp(-ce_loss)  # Probability of the correct token\n        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n\n        if self.reduction == \"mean\":\n            return focal_loss.mean()\n        elif self.reduction == \"sum\":\n            return focal_loss.sum()\n        return focal_loss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T20:11:51.319309Z","iopub.execute_input":"2024-12-20T20:11:51.320032Z","iopub.status.idle":"2024-12-20T20:11:51.328800Z","shell.execute_reply.started":"2024-12-20T20:11:51.319992Z","shell.execute_reply":"2024-12-20T20:11:51.327779Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"#5. Define the model ","metadata":{}},{"cell_type":"code","source":"import json\nimport logging\nimport math\nimport os\nimport random\nimport warnings\nfrom dataclasses import asdict\nfrom multiprocessing import Pool, cpu_count\nfrom pathlib import Path\n\n# import test_Rest14\n\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom tensorboardX import SummaryWriter\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\nfrom torch.utils.data.distributed import DistributedSampler\nfrom tqdm.auto import tqdm, trange\nfrom transformers import (\n    AdamW,\n    AutoConfig,\n    AutoModel,\n    AutoTokenizer,\n    BartConfig,\n    BartForConditionalGeneration,\n    BartTokenizer,\n    BertConfig,\n    BertForMaskedLM,\n    BertModel,\n    BertTokenizer,\n    CamembertConfig,\n    CamembertModel,\n    CamembertTokenizer,\n    DistilBertConfig,\n    DistilBertModel,\n    DistilBertTokenizer,\n    ElectraConfig,\n    ElectraModel,\n    ElectraTokenizer,\n    EncoderDecoderConfig,\n    EncoderDecoderModel,\n    LongformerConfig,\n    LongformerModel,\n    LongformerTokenizer,\n    MarianConfig,\n    MarianMTModel,\n    MarianTokenizer,\n    MobileBertConfig,\n    MobileBertModel,\n    MobileBertTokenizer,\n    PreTrainedModel,\n    PreTrainedTokenizer,\n    RobertaConfig,\n    RobertaModel,\n    RobertaTokenizer,\n    get_linear_schedule_with_warmup,\n    MBartConfig, MBartForConditionalGeneration\n)\n\nfrom simpletransformers.config.global_args import global_args\nfrom simpletransformers.config.model_args import Seq2SeqArgs\nfrom simpletransformers.seq2seq.seq2seq_utils import Seq2SeqDataset, SimpleSummarizationDataset\n\ntry:\n    import wandb\n\n    wandb_available = True\nexcept ImportError:\n    wandb_available = False\n\nlogger = logging.getLogger(__name__)\n\nMODEL_CLASSES = {\n    \"auto\": (AutoConfig, AutoModel, AutoTokenizer),\n    \"bart\": (BartConfig, BartForConditionalGeneration, BartTokenizer),\n    \"bert\": (BertConfig, BertModel, BertTokenizer),\n    \"camembert\": (CamembertConfig, CamembertModel, CamembertTokenizer),\n    \"distilbert\": (DistilBertConfig, DistilBertModel, DistilBertTokenizer),\n    \"electra\": (ElectraConfig, ElectraModel, ElectraTokenizer),\n    \"longformer\": (LongformerConfig, LongformerModel, LongformerTokenizer),\n    \"mobilebert\": (MobileBertConfig, MobileBertModel, MobileBertTokenizer),\n    \"marian\": (MarianConfig, MarianMTModel, MarianTokenizer),\n    \"roberta\": (RobertaConfig, RobertaModel, RobertaTokenizer),\n    \"mbart\" : (MBartConfig, MBartForConditionalGeneration, AutoTokenizer)\n}\n\n\nclass Seq2SeqModel:\n    def __init__(\n        self,\n        encoder_type=None,\n        encoder_name=None,\n        decoder_name=None,\n        encoder_decoder_type=None,\n        encoder_decoder_name=None,\n        config=None,\n        args=None,\n        use_cuda=True,\n        cuda_device=0,\n        **kwargs,\n    ):\n\n        \"\"\"\n        Initializes a Seq2SeqModel.\n\n        Args:\n            encoder_type (optional): The type of model to use as the encoder.\n            encoder_name (optional): The exact architecture and trained weights to use. This may be a Hugging Face Transformers compatible pre-trained model, a community model, or the path to a directory containing model files.\n            decoder_name (optional): The exact architecture and trained weights to use. This may be a Hugging Face Transformers compatible pre-trained model, a community model, or the path to a directory containing model files.\n                                    Must be the same \"size\" as the encoder model (base/base, large/large, etc.)\n            encoder_decoder_type (optional): The type of encoder-decoder model. (E.g. bart)\n            encoder_decoder_name (optional): The path to a directory containing the saved encoder and decoder of a Seq2SeqModel. (E.g. \"outputs/\") OR a valid BART or MarianMT model.\n            config (optional): A configuration file to build an EncoderDecoderModel.\n            args (optional): Default args will be used if this parameter is not provided. If provided, it should be a dict containing the args that should be changed in the default args.\n            use_cuda (optional): Use GPU if available. Setting to False will force model to use CPU only.\n            cuda_device (optional): Specific GPU that should be used. Will use the first available GPU by default.\n            **kwargs (optional): For providing proxies, force_download, resume_download, cache_dir and other options specific to the 'from_pretrained' implementation where this will be supplied.\n        \"\"\"  # noqa: ignore flake8\"\n\n        if not config:\n            # if not ((encoder_name and decoder_name) or encoder_decoder_name) and not encoder_type:\n            if not ((encoder_name and decoder_name) or encoder_decoder_name):\n                raise ValueError(\n                    \"You must specify a Seq2Seq config \\t OR \\t\"\n                    \"encoder_type, encoder_name, and decoder_name OR \\t \\t\"\n                    \"encoder_type and encoder_decoder_name\"\n                )\n            elif not (encoder_type or encoder_decoder_type):\n                raise ValueError(\n                    \"You must specify a Seq2Seq config \\t OR \\t\"\n                    \"encoder_type, encoder_name, and decoder_name \\t OR \\t\"\n                    \"encoder_type and encoder_decoder_name\"\n                )\n\n        self.args = self._load_model_args(encoder_decoder_name)\n        if isinstance(args, dict):\n            self.args.update_from_dict(args)\n        elif isinstance(args, Seq2SeqArgs):\n            self.args = args\n\n        if \"sweep_config\" in kwargs:\n            sweep_config = kwargs.pop(\"sweep_config\")\n            sweep_values = {key: value[\"value\"] for key, value in sweep_config.as_dict().items() if key != \"_wandb\"}\n            self.args.update_from_dict(sweep_values)\n\n        if self.args.manual_seed:\n            random.seed(self.args.manual_seed)\n            np.random.seed(self.args.manual_seed)\n            torch.manual_seed(self.args.manual_seed)\n            if self.args.n_gpu > 0:\n                torch.cuda.manual_seed_all(self.args.manual_seed)\n\n        if use_cuda:\n            if torch.cuda.is_available():\n                if cuda_device == -1:\n                    self.device = torch.device(\"cuda\")\n                else:\n                    self.device = torch.device(f\"cuda:{cuda_device}\")\n            else:\n                raise ValueError(\n                    \"'use_cuda' set to True when cuda is unavailable.\"\n                    \"Make sure CUDA is available or set use_cuda=False.\"\n                )\n        else:\n            self.device = \"cpu\"\n\n        self.results = {}\n\n        if not use_cuda:\n            self.args.fp16 = False\n\n        # config = EncoderDecoderConfig.from_encoder_decoder_configs(config, config)\n        if encoder_decoder_type:\n            config_class, model_class, tokenizer_class = MODEL_CLASSES[encoder_decoder_type]\n        else:\n            config_class, model_class, tokenizer_class = MODEL_CLASSES[encoder_type]\n\n        if encoder_decoder_type in [\"bart\", \"marian\",\"mbart\"]:\n            self.model = model_class.from_pretrained(encoder_decoder_name)\n            if encoder_decoder_type in [\"mbart\",\"bart\"]:\n                self.encoder_tokenizer = tokenizer_class.from_pretrained(encoder_decoder_name)\n            elif encoder_decoder_type == \"marian\":\n                if self.args.base_marian_model_name:\n                    self.encoder_tokenizer = tokenizer_class.from_pretrained(self.args.base_marian_model_name)\n                else:\n                    self.encoder_tokenizer = tokenizer_class.from_pretrained(encoder_decoder_name)\n            self.decoder_tokenizer = self.encoder_tokenizer\n            self.config = self.model.config\n        else:\n            if encoder_decoder_name:\n                # self.model = EncoderDecoderModel.from_pretrained(encoder_decoder_name)\n                self.model = EncoderDecoderModel.from_encoder_decoder_pretrained(\n                    os.path.join(encoder_decoder_name, \"encoder\"), os.path.join(encoder_decoder_name, \"decoder\")\n                )\n                self.model.encoder = model_class.from_pretrained(os.path.join(encoder_decoder_name, \"encoder\"))\n                self.model.decoder = BertForMaskedLM.from_pretrained(os.path.join(encoder_decoder_name, \"decoder\"))\n                self.encoder_tokenizer = tokenizer_class.from_pretrained(os.path.join(encoder_decoder_name, \"encoder\"))\n                self.decoder_tokenizer = BertTokenizer.from_pretrained(os.path.join(encoder_decoder_name, \"decoder\"))\n            else:\n                self.model = EncoderDecoderModel.from_encoder_decoder_pretrained(\n                    encoder_name, decoder_name, config=config\n                )\n                self.encoder_tokenizer = tokenizer_class.from_pretrained(encoder_name)\n                self.decoder_tokenizer = BertTokenizer.from_pretrained(decoder_name)\n            self.encoder_config = self.model.config.encoder\n            self.decoder_config = self.model.config.decoder\n\n        if self.args.wandb_project and not wandb_available:\n            warnings.warn(\"wandb_project specified but wandb is not available. Wandb disabled.\")\n            self.args.wandb_project = None\n\n        if encoder_decoder_name:\n            self.args.model_name = encoder_decoder_name\n\n            # # Checking if we are loading from a saved model or using a pre-trained model\n            # if not saved_model_args and encoder_decoder_type == \"marian\":\n            # Need to store base pre-trained model name to get the tokenizer when loading a saved model\n            self.args.base_marian_model_name = encoder_decoder_name\n\n        elif encoder_name and decoder_name:\n            self.args.model_name = encoder_name + \"-\" + decoder_name\n        else:\n            self.args.model_name = \"encoder-decoder\"\n\n        if encoder_decoder_type:\n            self.args.model_type = \"bart\"\n        elif encoder_type:\n            self.args.model_type = encoder_type + \"-bert\"\n        else:\n            self.args.model_type = \"encoder-decoder\"\n\n        self.focal_loss = SequenceFocalLoss(alpha=3.0, gamma=4.0, reduction=\"mean\")\n\n\n        print(self.args.model_type)\n    def train_model(\n        self, train_data,test_df, best_accuracy, output_dir=None, show_running_loss=True, args=None, eval_data=None, verbose=True, **kwargs,\n    ):\n        \"\"\"\n        Trains the model using 'train_data'\n\n        Args:\n            train_data: Pandas DataFrame containing the 2 columns - input_text, target_text.\n                        - input_text: The input text sequence.\n                        - target_text: The target text sequence\n            output_dir: The directory where model files will be saved. If not given, self.args.output_dir will be used.\n            show_running_loss (optional): Set to False to prevent running loss from being printed to console. Defaults to True.\n            args (optional): Optional changes to the args dict of the model. Any changes made will persist for the model.\n            eval_data (optional): A DataFrame against which evaluation will be performed when evaluate_during_training is enabled. Is required if evaluate_during_training is enabled.\n            **kwargs: Additional metrics that should be used. Pass in the metrics as keyword arguments (name of metric: function to use).\n                        A metric function should take in two parameters. The first parameter will be the true labels, and the second parameter will be the predictions. Both inputs\n                        will be lists of strings. Note that this will slow down training significantly as the predicted sequences need to be generated.\n\n        Returns:\n            None\n        \"\"\"  # noqa: ignore flake8\"\n\n        if args:\n            self.args.update_from_dict(args)\n\n        # if self.args.silent:\n        #     show_running_loss = False\n\n        if self.args.evaluate_during_training and eval_data is None:\n            raise ValueError(\n                \"evaluate_during_training is enabled but eval_data is not specified.\"\n                \" Pass eval_data to model.train_model() if using evaluate_during_training.\"\n            )\n\n        if not output_dir:\n            output_dir = self.args.output_dir\n\n        if os.path.exists(output_dir) and os.listdir(output_dir) and not self.args.overwrite_output_dir:\n            raise ValueError(\n                \"Output directory ({}) already exists and is not empty.\"\n                \" Set args.overwrite_output_dir = True to overcome.\".format(output_dir)\n            )\n\n        self._move_model_to_device()\n\n        train_dataset = self.load_and_cache_examples(train_data, verbose=verbose)\n\n        os.makedirs(output_dir, exist_ok=True)\n\n        global_step, tr_loss, best_accuracy = self.train(\n            train_dataset,\n            output_dir,\n            best_accuracy,\n            show_running_loss=show_running_loss,\n            eval_data=eval_data,\n            verbose=verbose,\n            **kwargs,\n        )\n\n        self._save_model(self.args.output_dir, model=self.model)\n\n        # model_to_save = self.model.module if hasattr(self.model, \"module\") else self.model\n        # model_to_save.save_pretrained(output_dir)\n        # self.encoder_tokenizer.save_pretrained(output_dir)\n        # self.decoder_tokenizer.save_pretrained(output_dir)\n        # torch.save(self.args, os.path.join(output_dir, \"training_args.bin\"))\n\n        if verbose:\n            logger.info(\" Training of {} model complete. Saved to {}.\".format(self.args.model_name, output_dir))\n\n        return best_accuracy\n\n    def train(\n        self, train_dataset, output_dir, best_accuracy, show_running_loss=True, eval_data=None, verbose=True, **kwargs,\n    ):\n        \"\"\"\n        Trains the model on train_dataset.\n\n        Utility function to be used by the train_model() method. Not intended to be used directly.\n        \"\"\"\n\n        model = self.model\n        args = self.args\n\n        tb_writer = SummaryWriter(logdir=args.tensorboard_dir)\n        train_sampler = RandomSampler(train_dataset)\n        train_dataloader = DataLoader(\n            train_dataset,\n            sampler=train_sampler,\n            batch_size=args.train_batch_size,\n            num_workers=self.args.dataloader_num_workers,\n        )\n\n        if args.max_steps > 0:\n            t_total = args.max_steps\n            args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n        else:\n            t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n\n        no_decay = [\"bias\", \"LayerNorm.weight\"]\n\n        optimizer_grouped_parameters = []\n        custom_parameter_names = set()\n        for group in self.args.custom_parameter_groups:\n            params = group.pop(\"params\")\n            custom_parameter_names.update(params)\n            param_group = {**group}\n            param_group[\"params\"] = [p for n, p in model.named_parameters() if n in params]\n            optimizer_grouped_parameters.append(param_group)\n\n        for group in self.args.custom_layer_parameters:\n            layer_number = group.pop(\"layer\")\n            layer = f\"layer.{layer_number}.\"\n            group_d = {**group}\n            group_nd = {**group}\n            group_nd[\"weight_decay\"] = 0.0\n            params_d = []\n            params_nd = []\n            for n, p in model.named_parameters():\n                if n not in custom_parameter_names and layer in n:\n                    if any(nd in n for nd in no_decay):\n                        params_nd.append(p)\n                    else:\n                        params_d.append(p)\n                    custom_parameter_names.add(n)\n            group_d[\"params\"] = params_d\n            group_nd[\"params\"] = params_nd\n\n            optimizer_grouped_parameters.append(group_d)\n            optimizer_grouped_parameters.append(group_nd)\n\n        if not self.args.train_custom_parameters_only:\n            optimizer_grouped_parameters.extend(\n                [\n                    {\n                        \"params\": [\n                            p\n                            for n, p in model.named_parameters()\n                            if n not in custom_parameter_names and not any(nd in n for nd in no_decay)\n                        ],\n                        \"weight_decay\": args.weight_decay,\n                    },\n                    {\n                        \"params\": [\n                            p\n                            for n, p in model.named_parameters()\n                            if n not in custom_parameter_names and any(nd in n for nd in no_decay)\n                        ],\n                        \"weight_decay\": 0.0,\n                    },\n                ]\n            )\n        \n        warmup_steps = math.ceil(t_total * args.warmup_ratio)\n        args.warmup_steps = warmup_steps if args.warmup_steps == 0 else args.warmup_steps\n        # print(optimizer_grouped_parameters)\n        # TODO: Use custom optimizer like with BertSum?\n        optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n        scheduler = get_linear_schedule_with_warmup(\n            optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n        )\n\n        if (\n            args.model_name\n            and os.path.isfile(os.path.join(args.model_name, \"optimizer.pt\"))\n            and os.path.isfile(os.path.join(args.model_name, \"scheduler.pt\"))\n        ):\n            # Load in optimizer and scheduler states\n            optimizer.load_state_dict(torch.load(os.path.join(args.model_name, \"optimizer.pt\")))\n            scheduler.load_state_dict(torch.load(os.path.join(args.model_name, \"scheduler.pt\")))\n\n        if args.n_gpu > 1:\n            model = torch.nn.DataParallel(model)\n\n        logger.info(\" Training started\")\n\n        global_step = 0\n        tr_loss, logging_loss = 0.0, 0.0\n        model.zero_grad()\n        train_iterator = trange(int(args.num_train_epochs), desc=\"Epoch\", disable=False, mininterval=0)\n        epoch_number = 0\n        best_eval_metric = None\n        early_stopping_counter = 0\n        steps_trained_in_current_epoch = 0\n        epochs_trained = 0\n\n        if args.model_name and os.path.exists(args.model_name):\n            try:\n                # set global_step to gobal_step of last saved checkpoint from model path\n                checkpoint_suffix = args.model_name.split(\"/\")[-1].split(\"-\")\n                if len(checkpoint_suffix) > 2:\n                    checkpoint_suffix = checkpoint_suffix[1]\n                else:\n                    checkpoint_suffix = checkpoint_suffix[-1]\n                global_step = int(checkpoint_suffix)\n                epochs_trained = global_step // (len(train_dataloader) // args.gradient_accumulation_steps)\n                steps_trained_in_current_epoch = global_step % (\n                    len(train_dataloader) // args.gradient_accumulation_steps\n                )\n\n                logger.info(\"   Continuing training from checkpoint, will skip to saved global_step\")\n                logger.info(\"   Continuing training from epoch %d\", epochs_trained)\n                logger.info(\"   Continuing training from global step %d\", global_step)\n                logger.info(\"   Will skip the first %d steps in the current epoch\", steps_trained_in_current_epoch)\n            except ValueError:\n                logger.info(\"   Starting fine-tuning.\")\n\n        if args.evaluate_during_training:\n            training_progress_scores = self._create_training_progress_scores(**kwargs)\n\n        if args.wandb_project:\n            wandb.init(project=args.wandb_project, config={**asdict(args)}, **args.wandb_kwargs)\n            wandb.watch(self.model)\n\n        if args.fp16:\n            from torch.cuda import amp\n\n            scaler = amp.GradScaler()\n\n        model.train()\n        for current_epoch in train_iterator:\n            if epochs_trained > 0:\n                epochs_trained -= 1\n                continue\n            train_iterator.set_description(f\"Epoch {epoch_number + 1} of {args.num_train_epochs}\")\n            batch_iterator = tqdm(\n                train_dataloader,\n                desc=f\"Running Epoch {epoch_number} of {args.num_train_epochs}\",\n                disable= False,\n                mininterval=0,\n            )\n            for step, batch in enumerate(batch_iterator):\n                if steps_trained_in_current_epoch > 0:\n                    steps_trained_in_current_epoch -= 1\n                    continue\n                # batch = tuple(t.to(device) for t in batch)\n\n                inputs = self._get_inputs_dict(batch)\n                if args.fp16:\n                    with amp.autocast():\n                        outputs = model(**inputs)\n                        # model outputs are always tuple in pytorch-transformers (see doc)\n                        logits = outputs[1]\n                        loss = self.focal_loss(logits, inputs[\"labels\"])\n                        #loss = outputs[0]\n                        \n                else:\n                    outputs = model(**inputs)\n                    #model outputs are always tuple in pytorch-transformers (see doc)\n                    logits = outputs[1]\n                    loss = self.focal_loss(logits, inputs[\"labels\"])\n                    #loss = outputs[0]\n\n                if args.n_gpu > 1:\n                    loss = loss.mean()  # mean() to average on multi-gpu parallel training\n\n                # print(f\"current_loss: {current_loss}\")\n                current_loss = loss.item()\n\n                if show_running_loss:\n                    batch_iterator.set_description(\n                        f\"Epochs {epoch_number}/{args.num_train_epochs}. Running Loss: {current_loss:9.4f}\"\n                    )\n\n                if args.gradient_accumulation_steps > 1:\n                    loss = loss / args.gradient_accumulation_steps\n\n                if args.fp16:\n                    scaler.scale(loss).backward()\n                else:\n                    loss.backward()\n\n                tr_loss += loss.item()\n                if (step + 1) % args.gradient_accumulation_steps == 0:\n                    if args.fp16:\n                        scaler.unscale_(optimizer)\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n\n                    if args.fp16:\n                        scaler.step(optimizer)\n                        scaler.update()\n                    else:\n                        optimizer.step()\n                    scheduler.step()  # Update learning rate schedule\n                    model.zero_grad()\n                    global_step += 1\n\n                    if args.logging_steps > 0 and global_step % args.logging_steps == 0:\n                        # Log metrics\n                        tb_writer.add_scalar(\"lr\", scheduler.get_lr()[0], global_step)\n                        tb_writer.add_scalar(\"loss\", (tr_loss - logging_loss) / args.logging_steps, global_step)\n                        logging_loss = tr_loss\n                        if args.wandb_project:\n                            wandb.log(\n                                {\n                                    \"Training loss\": current_loss,\n                                    \"lr\": scheduler.get_lr()[0],\n                                    \"global_step\": global_step,\n                                }\n                            )\n\n                    if args.save_steps > 0 and global_step % args.save_steps == 0:\n                        # Save model checkpoint\n                        output_dir_current = os.path.join(output_dir, \"checkpoint-{}\".format(global_step))\n\n                        self._save_model(output_dir_current, optimizer, scheduler, model=model)\n\n                    if args.evaluate_during_training and (\n                        args.evaluate_during_training_steps > 0\n                        and global_step % args.evaluate_during_training_steps == 0\n                    ):\n                        # Only evaluate when single GPU otherwise metrics may not average well\n                        results = self.eval_model(\n                            eval_data,\n                            verbose=verbose and args.evaluate_during_training_verbose,\n                            silent=args.evaluate_during_training_silent,\n                            **kwargs,\n                        )\n                        for key, value in results.items():\n                            tb_writer.add_scalar(\"eval_{}\".format(key), value, global_step)\n\n                        output_dir_current = os.path.join(output_dir, \"checkpoint-{}\".format(global_step))\n\n                        if args.save_eval_checkpoints:\n                            self._save_model(output_dir_current, optimizer, scheduler, model=model, results=results)\n\n                        training_progress_scores[\"global_step\"].append(global_step)\n                        training_progress_scores[\"train_loss\"].append(current_loss)\n                        for key in results:\n                            training_progress_scores[key].append(results[key])\n                        report = pd.DataFrame(training_progress_scores)\n                        report.to_csv(\n                            os.path.join(args.output_dir, \"training_progress_scores.csv\"), index=False,\n                        )\n\n                        if args.wandb_project:\n                            wandb.log(self._get_last_metrics(training_progress_scores))\n\n                        if not best_eval_metric:\n                            best_eval_metric = results[args.early_stopping_metric]\n                            if args.save_best_model:\n                                self._save_model(\n                                    args.best_model_dir, optimizer, scheduler, model=model, results=results\n                                )\n                        if best_eval_metric and args.early_stopping_metric_minimize:\n                            if results[args.early_stopping_metric] - best_eval_metric < args.early_stopping_delta:\n                                best_eval_metric = results[args.early_stopping_metric]\n                                if args.save_best_model:\n                                    self._save_model(\n                                        args.best_model_dir, optimizer, scheduler, model=model, results=results\n                                    )\n                                early_stopping_counter = 0\n                            else:\n                                if args.use_early_stopping:\n                                    if early_stopping_counter < args.early_stopping_patience:\n                                        early_stopping_counter += 1\n                                        if verbose:\n                                            logger.info(f\" No improvement in {args.early_stopping_metric}\")\n                                            logger.info(f\" Current step: {early_stopping_counter}\")\n                                            logger.info(f\" Early stopping patience: {args.early_stopping_patience}\")\n                                    else:\n                                        if verbose:\n                                            logger.info(f\" Patience of {args.early_stopping_patience} steps reached\")\n                                            logger.info(\" Training terminated.\")\n                                            train_iterator.close()\n                                        return global_step, tr_loss / global_step\n                        else:\n                            if results[args.early_stopping_metric] - best_eval_metric > args.early_stopping_delta:\n                                best_eval_metric = results[args.early_stopping_metric]\n                                if args.save_best_model:\n                                    self._save_model(\n                                        args.best_model_dir, optimizer, scheduler, model=model, results=results\n                                    )\n                                early_stopping_counter = 0\n                            else:\n                                if args.use_early_stopping:\n                                    if early_stopping_counter < args.early_stopping_patience:\n                                        early_stopping_counter += 1\n                                        if verbose:\n                                            logger.info(f\" No improvement in {args.early_stopping_metric}\")\n                                            logger.info(f\" Current step: {early_stopping_counter}\")\n                                            logger.info(f\" Early stopping patience: {args.early_stopping_patience}\")\n                                    else:\n                                        if verbose:\n                                            logger.info(f\" Patience of {args.early_stopping_patience} steps reached\")\n                                            logger.info(\" Training terminated.\")\n                                            train_iterator.close()\n                                        return global_step, tr_loss / global_step\n\n            epoch_number += 1\n            output_dir_current = os.path.join(output_dir, \"checkpoint-{}-epoch-{}\".format(global_step, epoch_number))\n            \n\n            accuracy = predict_val(model, self.device, test_df)\n            print(accuracy)\n            print('batch: '+str(args.train_batch_size)+' accumulation_steps: '+str(args.gradient_accumulation_steps)+\\\n                ' lr: '+str(args.learning_rate)+' epochs: '+str(args.num_train_epochs)+' epoch: '+str(epoch_number) + \" loss: \"+ str(tr_loss / global_step))\n            if accuracy > best_accuracy:\n                best_accuracy = accuracy\n                print('---test dataset----')\n                # test_acc = predict_test(model, self.device)\n                \n                # with open('./MAMS_best_accuracy.txt', 'a') as f0:\n                #     f0.writelines('batch: '+str(args.train_batch_size)+' accumulation_steps: '+str(args.gradient_accumulation_steps)+\\\n                #                   ' lr: '+str(args.learning_rate)+' epochs: '+str(args.num_train_epochs)+' epoch: '+str(epoch_number)+' val_accuracy: '+str(best_accuracy)+\\\n                #                   ' test_accuracy: '+str(test_acc)+'\\n')\n\n\n            if args.save_model_every_epoch or args.evaluate_during_training:\n                os.makedirs(output_dir_current, exist_ok=True)\n\n            if args.save_model_every_epoch:\n                self._save_model(output_dir_current, optimizer, scheduler, model=model)\n\n            if args.evaluate_during_training:\n                results = self.eval_model(\n                    eval_data,\n                    verbose=verbose and args.evaluate_during_training_verbose,\n                    silent=args.evaluate_during_training_silent,\n                    **kwargs,\n                )\n\n                if args.save_eval_checkpoints:\n                    self._save_model(output_dir_current, optimizer, scheduler, results=results)\n\n                training_progress_scores[\"global_step\"].append(global_step)\n                training_progress_scores[\"train_loss\"].append(current_loss)\n                for key in results:\n                    training_progress_scores[key].append(results[key])\n                report = pd.DataFrame(training_progress_scores)\n                report.to_csv(os.path.join(args.output_dir, \"training_progress_scores.csv\"), index=False)\n\n                if args.wandb_project:\n                    wandb.log(self._get_last_metrics(training_progress_scores))\n\n                if not best_eval_metric:\n                    best_eval_metric = results[args.early_stopping_metric]\n                    if args.save_best_model:\n                        self._save_model(args.best_model_dir, optimizer, scheduler, model=model, results=results)\n                if best_eval_metric and args.early_stopping_metric_minimize:\n                    if results[args.early_stopping_metric] - best_eval_metric < args.early_stopping_delta:\n                        best_eval_metric = results[args.early_stopping_metric]\n                        if args.save_best_model:\n                            self._save_model(args.best_model_dir, optimizer, scheduler, model=model, results=results)\n                        early_stopping_counter = 0\n                    else:\n                        if args.use_early_stopping and args.early_stopping_consider_epochs:\n                            if early_stopping_counter < args.early_stopping_patience:\n                                early_stopping_counter += 1\n                                if verbose:\n                                    logger.info(f\" No improvement in {args.early_stopping_metric}\")\n                                    logger.info(f\" Current step: {early_stopping_counter}\")\n                                    logger.info(f\" Early stopping patience: {args.early_stopping_patience}\")\n                            else:\n                                if verbose:\n                                    logger.info(f\" Patience of {args.early_stopping_patience} steps reached\")\n                                    logger.info(\" Training terminated.\")\n                                    train_iterator.close()\n                                return global_step, tr_loss / global_step\n                else:\n                    if results[args.early_stopping_metric] - best_eval_metric > args.early_stopping_delta:\n                        best_eval_metric = results[args.early_stopping_metric]\n                        if args.save_best_model:\n                            self._save_model(args.best_model_dir, optimizer, scheduler, model=model, results=results)\n                        early_stopping_counter = 0\n                    else:\n                        if args.use_early_stopping and args.early_stopping_consider_epochs:\n                            if early_stopping_counter < args.early_stopping_patience:\n                                early_stopping_counter += 1\n                                if verbose:\n                                    logger.info(f\" No improvement in {args.early_stopping_metric}\")\n                                    logger.info(f\" Current step: {early_stopping_counter}\")\n                                    logger.info(f\" Early stopping patience: {args.early_stopping_patience}\")\n                            else:\n                                if verbose:\n                                    logger.info(f\" Patience of {args.early_stopping_patience} steps reached\")\n                                    logger.info(\" Training terminated.\")\n                                    train_iterator.close()\n                                return global_step, tr_loss / global_step\n\n        return global_step, tr_loss / global_step, best_accuracy\n\n    def eval_model(self, eval_data, output_dir=None, verbose=True, silent=False, **kwargs):\n        \"\"\"\n        Evaluates the model on eval_data. Saves results to output_dir.\n\n        Args:\n            eval_data: Pandas DataFrame containing the 2 columns - input_text, target_text.\n                        - input_text: The input text sequence.\n                        - target_text: The target text sequence.\n            output_dir: The directory where model files will be saved. If not given, self.args.output_dir will be used.\n            verbose: If verbose, results will be printed to the console on completion of evaluation.\n            silent: If silent, tqdm progress bars will be hidden.\n            **kwargs: Additional metrics that should be used. Pass in the metrics as keyword arguments (name of metric: function to use).\n                        A metric function should take in two parameters. The first parameter will be the true labels, and the second parameter will be the predictions. Both inputs\n                        will be lists of strings. Note that this will slow down evaluation significantly as the predicted sequences need to be generated.\n        Returns:\n            results: Dictionary containing evaluation results.\n        \"\"\"  # noqa: ignore flake8\"\n\n        if not output_dir:\n            output_dir = self.args.output_dir\n\n        self._move_model_to_device()\n\n        eval_dataset = self.load_and_cache_examples(eval_data, evaluate=True, verbose=verbose, silent=silent)\n        os.makedirs(output_dir, exist_ok=True)\n\n        result = self.evaluate(eval_dataset, output_dir, verbose=verbose, silent=silent, **kwargs)\n        self.results.update(result)\n\n        if self.args.evaluate_generated_text:\n            to_predict = eval_data[\"input_text\"].tolist()\n            preds = self.predict(to_predict)\n\n            result = self.compute_metrics(eval_data[\"target_text\"].tolist(), preds, **kwargs)\n            self.results.update(result)\n\n        if verbose:\n            logger.info(self.results)\n\n        return self.results\n\n    def evaluate(self, eval_dataset, output_dir, verbose=True, silent=False, **kwargs):\n        \"\"\"\n        Evaluates the model on eval_dataset.\n\n        Utility function to be used by the eval_model() method. Not intended to be used directly.\n        \"\"\"\n\n        model = self.model\n        args = self.args\n        eval_output_dir = output_dir\n\n        results = {}\n\n        eval_sampler = SequentialSampler(eval_dataset)\n        eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n\n        if args.n_gpu > 1:\n            model = torch.nn.DataParallel(model)\n\n        eval_loss = 0.0\n        nb_eval_steps = 0\n        model.eval()\n\n        for batch in tqdm(eval_dataloader, disable=args.silent or silent, desc=\"Running Evaluation\"):\n            # batch = tuple(t.to(device) for t in batch)\n\n            inputs = self._get_inputs_dict(batch)\n            with torch.no_grad():\n                outputs = model(**inputs)\n                loss = outputs[0]\n                eval_loss += loss.mean().item()\n            nb_eval_steps += 1\n\n        eval_loss = eval_loss / nb_eval_steps\n\n        results[\"eval_loss\"] = eval_loss\n\n        output_eval_file = os.path.join(eval_output_dir, \"eval_results.txt\")\n        with open(output_eval_file, \"w\") as writer:\n            for key in sorted(results.keys()):\n                writer.write(\"{} = {}\\n\".format(key, str(results[key])))\n\n        return results\n\n    def predict(self, to_predict):\n        \"\"\"\n        Performs predictions on a list of text.\n\n        Args:\n            to_predict: A python list of text (str) to be sent to the model for prediction. Note that the prefix should be prepended to the text.\n\n        Returns:\n            preds: A python list of the generated sequences.\n        \"\"\"  # noqa: ignore flake8\"\n\n        self._move_model_to_device()\n\n        all_outputs = []\n        # Batching\n        for batch in [\n            to_predict[i : i + self.args.eval_batch_size] for i in range(0, len(to_predict), self.args.eval_batch_size)\n        ]:\n            if self.args.model_type == \"marian\":\n                input_ids = self.encoder_tokenizer.prepare_translation_batch(\n                    batch, max_length=self.args.max_seq_length, pad_to_max_length=True, return_tensors=\"pt\",\n                )[\"input_ids\"]\n            else:\n                input_ids = self.encoder_tokenizer.batch_encode_plus(\n                    batch, max_length=self.args.max_seq_length, pad_to_max_length=True, return_tensors=\"pt\",\n                )[\"input_ids\"]\n            input_ids = input_ids.to(self.device)\n\n            if self.args.model_type in [\"mbart\",\"bart\", \"marian\"]:\n                outputs = self.model.generate(\n                    input_ids=input_ids,\n                    num_beams=self.args.num_beams,\n                    max_length=self.args.max_length,\n                    length_penalty=self.args.length_penalty,\n                    early_stopping=self.args.early_stopping,\n                    repetition_penalty=self.args.repetition_penalty,\n                    do_sample=self.args.do_sample,\n                    top_k=self.args.top_k,\n                    top_p=self.args.top_p,\n                    num_return_sequences=self.args.num_return_sequences,\n                )\n            else:\n                outputs = self.model.generate(\n                    input_ids=input_ids,\n                    decoder_start_token_id=self.model.config.decoder.pad_token_id,\n                    num_beams=self.args.num_beams,\n                    max_length=self.args.max_length,\n                    length_penalty=self.args.length_penalty,\n                    early_stopping=self.args.early_stopping,\n                    repetition_penalty=self.args.repetition_penalty,\n                    do_sample=self.args.do_sample,\n                    top_k=self.args.top_k,\n                    top_p=self.args.top_p,\n                    num_return_sequences=self.args.num_return_sequences,\n                )\n\n            all_outputs.extend(outputs.cpu().numpy())\n\n        if self.args.use_multiprocessed_decoding:\n            self.model.to(\"cpu\")\n            with Pool(self.args.process_count) as p:\n                outputs = list(\n                    tqdm(\n                        p.imap(self._decode, all_outputs, chunksize=self.args.multiprocessing_chunksize),\n                        total=len(all_outputs),\n                        desc=\"Decoding outputs\",\n                        disable=self.args.silent,\n                    )\n                )\n            self._move_model_to_device()\n        else:\n            outputs = [\n                self.decoder_tokenizer.decode(output_id, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n                for output_id in all_outputs\n            ]\n\n        if self.args.num_return_sequences > 1:\n            return [\n                outputs[i : i + self.args.num_return_sequences]\n                for i in range(0, len(outputs), self.args.num_return_sequences)\n            ]\n        else:\n            return outputs\n\n    def _decode(self, output_id):\n        return self.decoder_tokenizer.decode(output_id, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n\n    def compute_metrics(self, labels, preds, **kwargs):\n        \"\"\"\n        Computes the evaluation metrics for the model predictions.\n\n        Args:\n            labels: List of target sequences\n            preds: List of model generated outputs\n            **kwargs: Custom metrics that should be used. Pass in the metrics as keyword arguments (name of metric: function to use).\n                        A metric function should take in two parameters. The first parameter will be the true labels, and the second parameter will be the predictions. Both inputs\n                        will be lists of strings. Note that this will slow down evaluation significantly as the predicted sequences need to be generated.\n\n        Returns:\n            result: Dictionary containing evaluation results.\n        \"\"\"  # noqa: ignore flake8\"\n        # assert len(labels) == len(preds)\n\n        results = {}\n        for metric, func in kwargs.items():\n            results[metric] = func(labels, preds)\n\n        return results\n\n    def load_and_cache_examples(self, data, evaluate=False, no_cache=False, verbose=True, silent=False):\n        \"\"\"\n        Creates a T5Dataset from data.\n\n        Utility function for train() and eval() methods. Not intended to be used directly.\n        \"\"\"\n\n        encoder_tokenizer = self.encoder_tokenizer\n        decoder_tokenizer = self.decoder_tokenizer\n        args = self.args\n\n        if not no_cache:\n            no_cache = args.no_cache\n\n        if not no_cache:\n            os.makedirs(self.args.cache_dir, exist_ok=True)\n\n        mode = \"dev\" if evaluate else \"train\"\n\n        if args.dataset_class:\n            CustomDataset = args.dataset_class\n            return CustomDataset(encoder_tokenizer, decoder_tokenizer, args, data, mode)\n        else:\n            if args.model_type in [\"mbart\",\"bart\", \"marian\"]:\n                return SimpleSummarizationDataset(encoder_tokenizer, self.args, data, mode)\n            else:\n                return Seq2SeqDataset(encoder_tokenizer, decoder_tokenizer, self.args, data, mode,)\n\n    def _create_training_progress_scores(self, **kwargs):\n        extra_metrics = {key: [] for key in kwargs}\n        training_progress_scores = {\n            \"global_step\": [],\n            \"eval_loss\": [],\n            \"train_loss\": [],\n            **extra_metrics,\n        }\n\n        return training_progress_scores\n\n    def _get_last_metrics(self, metric_values):\n        return {metric: values[-1] for metric, values in metric_values.items()}\n\n    def _save_model(self, output_dir=None, optimizer=None, scheduler=None, model=None, results=None):\n        if not output_dir:\n            output_dir = self.args.output_dir\n        os.makedirs(output_dir, exist_ok=True)\n\n        logger.info(f\"Saving model into {output_dir}\")\n\n        if model and not self.args.no_save:\n            # Take care of distributed/parallel training\n            model_to_save = model.module if hasattr(model, \"module\") else model\n            self._save_model_args(output_dir)\n\n            if self.args.model_type in [\"bart\", \"marian\"]:\n                os.makedirs(os.path.join(output_dir), exist_ok=True)\n                model_to_save.save_pretrained(output_dir)\n                self.config.save_pretrained(output_dir)\n                if self.args.model_type == \"bart\":\n                    self.encoder_tokenizer.save_pretrained(output_dir)\n            else:\n                os.makedirs(os.path.join(output_dir, \"encoder\"), exist_ok=True)\n                os.makedirs(os.path.join(output_dir, \"decoder\"), exist_ok=True)\n                self.encoder_config.save_pretrained(os.path.join(output_dir, \"encoder\"))\n                self.decoder_config.save_pretrained(os.path.join(output_dir, \"decoder\"))\n\n                model_to_save = (\n                    self.model.encoder.module if hasattr(self.model.encoder, \"module\") else self.model.encoder\n                )\n                model_to_save.save_pretrained(os.path.join(output_dir, \"encoder\"))\n\n                model_to_save = (\n                    self.model.decoder.module if hasattr(self.model.decoder, \"module\") else self.model.decoder\n                )\n\n                model_to_save.save_pretrained(os.path.join(output_dir, \"decoder\"))\n\n                self.encoder_tokenizer.save_pretrained(os.path.join(output_dir, \"encoder\"))\n                self.decoder_tokenizer.save_pretrained(os.path.join(output_dir, \"decoder\"))\n\n            torch.save(self.args, os.path.join(output_dir, \"training_args.bin\"))\n            if optimizer and scheduler and self.args.save_optimizer_and_scheduler:\n                torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n                torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n\n        if results:\n            output_eval_file = os.path.join(output_dir, \"eval_results.txt\")\n            with open(output_eval_file, \"w\") as writer:\n                for key in sorted(results.keys()):\n                    writer.write(\"{} = {}\\n\".format(key, str(results[key])))\n\n    def _move_model_to_device(self):\n        self.model.to(self.device)\n\n    def _get_inputs_dict(self, batch):\n        device = self.device\n        if self.args.model_type in [\"mbart\",\"bart\", \"marian\"]:\n            pad_token_id = self.encoder_tokenizer.pad_token_id\n            source_ids, source_mask, y = batch[\"source_ids\"], batch[\"source_mask\"], batch[\"target_ids\"]\n            y_ids = y[:, :-1].contiguous()\n            lm_labels = y[:, 1:].clone()\n            lm_labels[y[:, 1:] == pad_token_id] = -100\n\n            inputs = {\n                \"input_ids\": source_ids.to(device),\n                \"attention_mask\": source_mask.to(device),\n                \"decoder_input_ids\": y_ids.to(device),\n                \"labels\": lm_labels.to(device),\n            }\n        else:\n            lm_labels = batch[1]\n            lm_labels_masked = lm_labels.clone()\n            lm_labels_masked[lm_labels_masked == self.decoder_tokenizer.pad_token_id] = -100\n\n            inputs = {\n                \"input_ids\": batch[0].to(device),\n                \"decoder_input_ids\": lm_labels.to(device),\n                \"labels\": lm_labels_masked.to(device),\n            }\n\n        return inputs\n\n    def _save_model_args(self, output_dir):\n        os.makedirs(output_dir, exist_ok=True)\n        self.args.save(output_dir)\n\n    def _load_model_args(self, input_dir):\n        args = Seq2SeqArgs()\n        args.load(input_dir)\n        return args\n\n    def get_named_parameters(self):\n        return [n for n, p in self.model.named_parameters()]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T20:11:51.330057Z","iopub.execute_input":"2024-12-20T20:11:51.330293Z","iopub.status.idle":"2024-12-20T20:11:58.466731Z","shell.execute_reply.started":"2024-12-20T20:11:51.330262Z","shell.execute_reply":"2024-12-20T20:11:58.465870Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"#6. Train the model","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n# logging.basicConfig(level=logging.INFO)\n# transformers_logger = logging.getLogger(\"transformers\")\n# transformers_logger.setLevel(logging.WARNING)\n\n\n# train_df = pd.DataFrame(train_data, columns=[\"input_text\", \"target_text\"])\n\n# steps = [1, 2, 3, 4, 6]\n# learing_rates = [4e-5, 2e-5, 1e-5, 3e-5]\nsteps = [1]\nlearing_rates = [4e-5]\n\n\n\nbest_accuracy = 0\nfor lr in learing_rates:\n    for step in steps:\n        model_args = {\n            \"reprocess_input_data\": True,\n            \"overwrite_output_dir\": True,\n            \"max_seq_length\": 128,\n            \"train_batch_size\": 16*4,\n            \"num_train_epochs\": 15,\n            \"save_eval_checkpoints\": False,\n            \"save_model_every_epoch\": False,\n            \"evaluate_during_training\": False,\n            \"evaluate_generated_text\": False,\n            \"evaluate_during_training_verbose\": False,\n            \"use_multiprocessing\": False,\n            \"max_length\": 64,\n            \"manual_seed\": 42,\n            \"gradient_accumulation_steps\": step,\n            \"learning_rate\":  lr,\n            \"save_steps\": 99999999999999,\n        }\n\n        # Initialize model\n        model = Seq2SeqModel(\n            encoder_decoder_type=\"mbart\",\n            encoder_decoder_name=\"vinai/bartpho-syllable-base\",\n            args=model_args,\n            use_cuda = True\n        )\n\n        # Train the model\n        best_accuracy = model.train_model(train_df, test_df_small, best_accuracy)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#7. Load the latest model checkpoint and generate the answer for the test file","metadata":{}},{"cell_type":"code","source":"to_predict = test_df[\"input_text\"].tolist()[0]\npreds = model.predict(to_predict)","metadata":{"trusted":true,"execution":{"execution_failed":"2024-12-20T20:16:55.873Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Load model \nfrom transformers import MBartForConditionalGeneration # Replace with your specific model class\n\nfrom safetensors.torch import load_file\n\n# Path to the .safetensors file\nmodel_path = \"/kaggle/working/outputs/model.safetensors\"\n\n# Load the saved state_dict from the .safetensors file\nstate_dict = load_file(model_path)\n\n# Initialize the model architecture\n# Replace 'AutoModel.from_pretrained' with your specific architecture\nmodel_new = MBartForConditionalGeneration.from_pretrained(\"vinai/bartpho-syllable-base\", state_dict=state_dict)","metadata":{"trusted":true,"execution":{"execution_failed":"2024-12-20T20:16:55.874Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_answer(model, device, test_df, categories_list):\n    candidate_list = [\"none\", \"positive\", \"negative\", \"neutral\"]\n\n    # model = BartForConditionalGeneration.from_pretrained('./outputs/checkpoint-513-epoch-19')\n    model.eval()\n    model.config.use_cache = False\n    tokenizer = AutoTokenizer.from_pretrained('vinai/bartpho-syllable-base')\n    count = 0\n    total = 0\n    predictions = []\n    for index, row in test_df.iterrows():\n        x, labels = row[\"input_text\"],row[\"labels\"]\n        input_ids = tokenizer([x] * 4, return_tensors='pt')['input_ids']\n        #labels_aspect = {label[0] : label[1] for label in labels}\n        #labels_idx = [candidate_list.index(labels_aspect[category]) if category in labels_aspect.keys() else 0 for category in categories_list)\n        prediction = []\n        for category in categories_list:\n            score_list = []\n            target_list = [f\"Cảm xúc của hạng mục {category} là {candi}.\" for candi in\n                            candidate_list]\n            \n            output_ids = tokenizer(target_list, return_tensors='pt', padding=True, truncation=True)['input_ids']\n            with torch.no_grad():\n                output = model(input_ids=input_ids.to(device), decoder_input_ids=output_ids.to(device))[0]\n                logits = output.softmax(dim=-1).to('cpu').numpy()\n            for i in range(4):\n                score = 1\n                for j in range(logits[i].shape[0] - 2):\n                    score *= logits[i][j][output_ids[i][j + 1]]\n                score_list.append(score)\n                \n            predict_polarity = np.argmax(score_list)\n            total += 1\n            if predict_polarity > 0:\n                prediction.append((category,candidate_list[predict_polarity]))\n\n        predictions.append((x, prediction)) \n\n    return predictions\n\n    ","metadata":{"trusted":true,"execution":{"execution_failed":"2024-12-20T20:16:55.875Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_new.to(model.device)\nanswer = generate_answer(model_new, model.device, test_df, categories_list)","metadata":{"trusted":true,"execution":{"execution_failed":"2024-12-20T20:16:55.875Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Export to file text\ndef format_reviews(data):\n    formatted_reviews = []\n    for idx, (review, aspects) in enumerate(data, start=1):\n        review_cleaned = review.replace(\"_\", \" \").strip()\n        if aspects:  # If aspects list is not empty\n            aspects_formatted = \", \".join(f\"{{{aspect[0]}, {aspect[1]}}}\" for aspect in aspects)\n        else:  # If aspects list is empty\n            aspects_formatted = \"{}\"\n        formatted_review = f\"#{idx}\\n{review_cleaned}\\n{aspects_formatted}\"\n        formatted_reviews.append(formatted_review)\n    return \"\\n\\n\".join(formatted_reviews)\n\nformatted_text = format_reviews(answer)\n\n# Save the file in the current directory\noutput_file = \"formatted_reviews.txt\"\nwith open(output_file, \"w\", encoding=\"utf-8\") as file:\n    file.write(formatted_text)","metadata":{"trusted":true,"execution":{"execution_failed":"2024-12-20T20:16:55.875Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}